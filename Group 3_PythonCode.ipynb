{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group # 3 \n",
    "\n",
    "Dear Professor Wang\n",
    "\n",
    "As we discussed, we only selected three of eight methods of web scraping and data cleaning due to the overwhelming amount of python codes our group experimented.  Appologies for so many jupyter cells. All of my teammates worked so hard on the project.\n",
    "\n",
    "Our Approach to the project:\n",
    "    1. Divided the group work by city, each one is responsible for one city, total 8 cities.\n",
    "    2. Each one worked on web scraping for their city.\n",
    "    3. Each one cleaned their data according to the cleaning guidelines defined by the group\n",
    "    4. We combined the data from each city, about 1100 rows\n",
    "    5. We performed data cleaning\n",
    "\n",
    "We were aware of disadvantages of this approach,it is not efficient; however, everyone practised a lot.  \n",
    "    \n",
    "Website used:  MLSlisting.com\n",
    "\n",
    "WARNING: Some links are no longer valid, this caused some codes to fail\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Method & Data Cleaning 1 - Pleasanton City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "# For ignoring SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Real Estate & Homes for Sale | MLSListings.com | MLSListings\n"
     ]
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "print(soup.title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_items=soup.find_all(class_= \"card-title font-weight-bold listing-address mb-25\")\n",
    "#print(address_items)\n",
    "address_list1=[add.get_text() for add in address_items]\n",
    "len(address_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_items=soup.find_all(class_= \"card-title font-weight-bold listing-address mb-25\")\n",
    "#print(address_items)\n",
    "address_list2=[add.get_text() for add in address_items]\n",
    "len(address_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "address_items=soup.find_all(class_= \"card-title font-weight-bold listing-address mb-25\")\n",
    "#print(address_items)\n",
    "address_list3=[add.get_text() for add in address_items]\n",
    "len(address_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "address_items=soup.find_all(class_= \"card-title font-weight-bold listing-address mb-25\")\n",
    "#print(address_items)\n",
    "address_list4=[add.get_text() for add in address_items]\n",
    "len(address_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "address_items=soup.find_all(class_= \"card-title font-weight-bold listing-address mb-25\")\n",
    "#print(address_items)\n",
    "address_list5=[add.get_text() for add in address_items]\n",
    "len(address_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "address_items=soup.find_all(class_= \"card-title font-weight-bold listing-address mb-25\")\n",
    "#print(address_items)\n",
    "address_list6=[add.get_text() for add in address_items]\n",
    "len(address_list6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine address lists\n",
    "address_list=address_list1+address_list2+address_list3+address_list4+address_list5+address_list6\n",
    "len(address_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Full Address]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame({\"Full Address\":address_list})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive Sold Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soldprice_items=soup.find_all(class_= \"font-weight-bold listing-price d-block pull-left pr-25\")\n",
    "#print(soldprice_items[0].get_text())\n",
    "soldprice_list1=[sp.get_text() for sp in soldprice_items]\n",
    "len(soldprice_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "soldprice_items=soup.find_all(class_= \"font-weight-bold listing-price d-block pull-left pr-25\")\n",
    "#print(soldprice_items[0].get_text())\n",
    "soldprice_list2=[sp.get_text() for sp in soldprice_items]\n",
    "len(soldprice_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "soldprice_items=soup.find_all(class_= \"font-weight-bold listing-price d-block pull-left pr-25\")\n",
    "#print(soldprice_items[0].get_text())\n",
    "soldprice_list3=[sp.get_text() for sp in soldprice_items]\n",
    "len(soldprice_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "soldprice_items=soup.find_all(class_= \"font-weight-bold listing-price d-block pull-left pr-25\")\n",
    "#print(soldprice_items[0].get_text())\n",
    "soldprice_list4=[sp.get_text() for sp in soldprice_items]\n",
    "len(soldprice_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "soldprice_items=soup.find_all(class_= \"font-weight-bold listing-price d-block pull-left pr-25\")\n",
    "#print(soldprice_items[0].get_text())\n",
    "soldprice_list5=[sp.get_text() for sp in soldprice_items]\n",
    "len(soldprice_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "soldprice_items=soup.find_all(class_= \"font-weight-bold listing-price d-block pull-left pr-25\")\n",
    "#print(soldprice_items[0].get_text())\n",
    "soldprice_list6=[sp.get_text() for sp in soldprice_items]\n",
    "len(soldprice_list6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine SoldPrice List\n",
    "soldPrice_list=soldprice_list1+soldprice_list2+soldprice_list3+soldprice_list4+soldprice_list5+soldprice_list6\n",
    "len(soldPrice_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Address</th>\n",
       "      <th>Sold Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Full Address, Sold Price]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame({\"Full Address\":address_list, 'Sold Price':soldPrice_list})\n",
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive Sold Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "SoldDate_items=soup.find_all(class_= \"listing-statusd-block pull-left pl-50 pr-1 status-marker status-closed\")\n",
    "SoldDate_list1=[sp.get_text().strip(\"ld on Sold on\") for sp in SoldDate_items]\n",
    "len(SoldDate_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "SoldDate_items=soup.find_all(class_= \"listing-statusd-block pull-left pl-50 pr-1 status-marker status-closed\")\n",
    "SoldDate_list2=[sp.get_text().strip(\"ld on Sold on\") for sp in SoldDate_items]\n",
    "#SoldDate_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "SoldDate_items=soup.find_all(class_= \"listing-statusd-block pull-left pl-50 pr-1 status-marker status-closed\")\n",
    "SoldDate_list3=[sp.get_text().strip(\"ld on Sold on\") for sp in SoldDate_items]\n",
    "len(SoldDate_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "SoldDate_items=soup.find_all(class_= \"listing-statusd-block pull-left pl-50 pr-1 status-marker status-closed\")\n",
    "SoldDate_list4=[sp.get_text().strip(\"ld on Sold on\") for sp in SoldDate_items]\n",
    "len(SoldDate_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "SoldDate_items=soup.find_all(class_= \"listing-statusd-block pull-left pl-50 pr-1 status-marker status-closed\")\n",
    "SoldDate_list5=[sp.get_text().strip(\"ld on Sold on\") for sp in SoldDate_items]\n",
    "len(SoldDate_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "SoldDate_items=soup.find_all(class_= \"listing-statusd-block pull-left pl-50 pr-1 status-marker status-closed\")\n",
    "SoldDate_list6=[sp.get_text().strip(\"ld on Sold on\") for sp in SoldDate_items]\n",
    "len(SoldDate_list6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine SoldDate List\n",
    "SoldDate_list=SoldDate_list1+SoldDate_list2+SoldDate_list3+SoldDate_list4+SoldDate_list5+SoldDate_list6\n",
    "len(SoldDate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Address</th>\n",
       "      <th>Sold Price</th>\n",
       "      <th>Sold Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Full Address, Sold Price, Sold Date]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame({\"Full Address\":address_list, 'Sold Price':soldPrice_list, 'Sold Date': SoldDate_list})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive House Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "HouseType_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base listing-type mb-25\")\n",
    "HouseType_list1=[sp.get_text().strip() for sp in HouseType_items]\n",
    "#HouseType_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "HouseType_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base listing-type mb-25\")\n",
    "HouseType_list2=[sp.get_text().strip() for sp in HouseType_items]\n",
    "#HouseType_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "HouseType_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base listing-type mb-25\")\n",
    "HouseType_list3=[sp.get_text().strip() for sp in HouseType_items]\n",
    "#HouseType_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "HouseType_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base listing-type mb-25\")\n",
    "HouseType_list4=[sp.get_text().strip() for sp in HouseType_items]\n",
    "#HouseType_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "HouseType_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base listing-type mb-25\")\n",
    "HouseType_list5=[sp.get_text().strip() for sp in HouseType_items]\n",
    "len(HouseType_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "HouseType_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base listing-type mb-25\")\n",
    "HouseType_list6=[sp.get_text().strip() for sp in HouseType_items]\n",
    "#HouseType_list6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine HouseType List\n",
    "HouseType_list=HouseType_list1+HouseType_list2+HouseType_list3+HouseType_list4+HouseType_list5+HouseType_list6\n",
    "len(HouseType_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Address</th>\n",
       "      <th>Sold Price</th>\n",
       "      <th>Sold Date</th>\n",
       "      <th>House Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Full Address, Sold Price, Sold Date, House Type]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame({\"Full Address\":address_list, 'Sold Price':soldPrice_list, 'Sold Date': SoldDate_list,\"House Type\":HouseType_list})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive Bed, Bath,Sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]\n",
    "fact_list1=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2==0:\n",
    "        fact_list1.append(data_fact_1[i])\n",
    "#fact_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]   # Remove empty list\n",
    "fact_list2=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2==0:\n",
    "        fact_list2.append(data_fact_1[i])\n",
    "#fact_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e] # Remove empty list\n",
    "fact_list3=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2==0:\n",
    "        fact_list3.append(data_fact_1[i])\n",
    "#fact_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e] # Remove empty list\n",
    "fact_list4=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2==0:\n",
    "        fact_list4.append(data_fact_1[i])\n",
    "#fact_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e] # Remove empty list\n",
    "fact_list5=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2==0:\n",
    "        fact_list5.append(data_fact_1[i])\n",
    "#fact_list5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e] # Remove empty list\n",
    "fact_list6=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2==0:\n",
    "        fact_list6.append(data_fact_1[i])\n",
    "#fact_list6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine fact_part1 list\n",
    "fact_part1=fact_list1+fact_list2+fact_list3+fact_list4+fact_list5+fact_list6\n",
    "len(fact_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bed</th>\n",
       "      <th>Bath</th>\n",
       "      <th>Sqft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Bed, Bath, Sqft]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "df2=pd.DataFrame(fact_part1, columns=['Bed',\"Bath\",\"Sqft\"])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive Lot, Garage, Year Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e] # Remove empty list\n",
    "fact_list7=[] \n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2!=0:\n",
    "        fact_list7.append(data_fact_1[i])\n",
    "#fact_list7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]  # Remove empty list\n",
    "fact_list8=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2!=0:\n",
    "        fact_list8.append(data_fact_1[i])\n",
    "#fact_list8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())  \n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]   # Remove empty list\n",
    "fact_list9=[]\n",
    "for i in range(len(data_fact_1)):   \n",
    "    if i%2!=0:\n",
    "        fact_list9.append(data_fact_1[i])\n",
    "#fact_list9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]   # Remove empty list\n",
    "fact_list10=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2!=0:\n",
    "        fact_list10.append(data_fact_1[i])\n",
    "#fact_list10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]  # Remove empty list\n",
    "fact_list11=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2!=0:\n",
    "        fact_list11.append(data_fact_1[i])\n",
    "#fact_list11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"listing-info clearfix mb-25\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('span',attrs={\"class\":\"font-weight-bold info-item-value d-block pull-left pr-25\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "data_fact\n",
    "data_fact_1 = [e for e in data_fact if e]   # Remove empty list\n",
    "fact_list12=[]\n",
    "for i in range(len(data_fact_1)):\n",
    "    if i%2!=0:\n",
    "        fact_list12.append(data_fact_1[i])\n",
    "#fact_list12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine fact_part2 list\n",
    "fact_part2=fact_list7+fact_list8+fact_list9+fact_list10+fact_list11+fact_list12\n",
    "len(fact_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lot</th>\n",
       "      <th>Garage</th>\n",
       "      <th>Year Built</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Lot, Garage, Year Built]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "df3=pd.DataFrame(fact_part2, columns=['Lot',\"Garage\",\"Year Built\"])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "Broker_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base mb-25 listing-broker\")\n",
    "Broker_list1=[sp.get_text().strip('Broker:\\n') for sp in Broker_items]\n",
    "#Broker_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "Broker_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base mb-25 listing-broker\")\n",
    "Broker_list2=[sp.get_text().strip('Broker:\\n') for sp in Broker_items]\n",
    "#Broker_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "Broker_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base mb-25 listing-broker\")\n",
    "Broker_list3=[sp.get_text().strip('Broker:\\n') for sp in Broker_items]\n",
    "#Broker_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "Broker_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base mb-25 listing-broker\")\n",
    "#print(soldprice_items[4].get_text())\n",
    "Broker_list4=[sp.get_text().strip('Broker:\\n') for sp in Broker_items]\n",
    "#Broker_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "Broker_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base mb-25 listing-broker\")\n",
    "#print(soldprice_items[4].get_text())\n",
    "Broker_list5=[sp.get_text().strip('Broker:\\n') for sp in Broker_items]\n",
    "#Broker_list5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "Broker_items=soup.find_all(class_=\"listing-info clearfix font-size-sm line-height-base mb-25 listing-broker\")\n",
    "#print(soldprice_items[4].get_text())\n",
    "Broker_list6=[sp.get_text().strip('Broker:\\n') for sp in Broker_items]\n",
    "#Broker_list6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine broker list\n",
    "BrokerList=Broker_list1+Broker_list2+Broker_list3+Broker_list4+Broker_list5+Broker_list6\n",
    "len(BrokerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full Address</th>\n",
       "      <th>Sold Price</th>\n",
       "      <th>Sold Date</th>\n",
       "      <th>House Type</th>\n",
       "      <th>Broker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Full Address, Sold Price, Sold Date, House Type, Broker]\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "import pandas as pd\n",
    "df1=pd.DataFrame({\"Full Address\":address_list, 'Sold Price':soldPrice_list, 'Sold Date': SoldDate_list,\"House Type\":HouseType_list,'Broker': BrokerList})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retreive HOA, School District, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/property/be40891726/1498-arcangel-ter-pleasanton-ca-94588/10412887\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "\n",
    "allrows=soup.find_all('div', attrs={\"class\":\"card-block p-0\"})\n",
    "data_fact=[]\n",
    "for row in allrows:\n",
    "    row_list=row.find_all('p',attrs={\"class\":\"card-text font-size-midr line-height-xl\"})\n",
    "    datarow=[]\n",
    "    for cell in row_list:\n",
    "        datarow.append(cell.get_text())\n",
    "    data_fact.append(datarow)\n",
    "#data_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the website believe that you are accessing it using a mozilla browser\n",
    "url=\"https://www.mlslistings.com/search/result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/1\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "links_with_text1=[]\n",
    "for a in soup.find_all('a', href=True,text=True): \n",
    "        links_with_text1.append(a['href'])\n",
    "links=links_with_text1\n",
    "link1=links[10:41]\n",
    "len(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/2?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "links_with_text1=[]\n",
    "for a in soup.find_all('a', href=True,text=True): \n",
    "        links_with_text1.append(a['href'])\n",
    "links=links_with_text1\n",
    "link2=links[10:41]\n",
    "len(link2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/3?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "links_with_text1=[]\n",
    "for a in soup.find_all('a', href=True,text=True): \n",
    "        links_with_text1.append(a['href'])\n",
    "links=links_with_text1\n",
    "link3=links[10:41]\n",
    "len(link3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/4?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "links_with_text1=[]\n",
    "for a in soup.find_all('a', href=True,text=True): \n",
    "        links_with_text1.append(a['href'])\n",
    "links=links_with_text1\n",
    "link4=links[10:41]\n",
    "len(link4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/5?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "links_with_text1=[]\n",
    "for a in soup.find_all('a', href=True,text=True): \n",
    "        links_with_text1.append(a['href'])\n",
    "links=links_with_text1\n",
    "link5=links[10:41]\n",
    "len(link5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://www.mlslistings.com/Search/Result/d03e9cc3-91ee-42f4-bb7a-7a24cd9014a4/6?view=list\"\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "# Creating a BeautifulSoup object of the html page for easy extraction of data.\n",
    "soup = BS(webpage,'html.parser')\n",
    "links_with_text1=[]\n",
    "for a in soup.find_all('a', href=True,text=True): \n",
    "        links_with_text1.append(a['href'])\n",
    "links=links_with_text1\n",
    "link6=links[10:41]\n",
    "len(link6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine links_full list\n",
    "links_full=link1+link2+link3+link4+link5+link6\n",
    "len(links_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list=[]\n",
    "for a in range (len(links_full)):\n",
    "    element_modified=\"https://www.mlslistings.com\"+links_full[a]   # modify the links\n",
    "    final_list.append(element_modified)\n",
    "len(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidURL",
     "evalue": "URL can't contain control characters. '/search/result/city/Apple Valley' (found at least ' ')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-38960f437c58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mozilla/5.0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mwebpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mallrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"card-text font-size-midr line-height-xl\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 543\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1358\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1360\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1317\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1318\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1242\u001b[0m                 encode_chunked=False):\n\u001b[0;32m   1243\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mskips\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'skip_accept_encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mskips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m         \u001b[1;31m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mputrequest\u001b[1;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_contains_disallowed_url_pchar_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m             raise InvalidURL(f\"URL can't contain control characters. {url!r} \"\n\u001b[0m\u001b[0;32m   1118\u001b[0m                              f\"(found at least {match.group()!r})\")\n\u001b[0;32m   1119\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s %s %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_http_vsn_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidURL\u001b[0m: URL can't contain control characters. '/search/result/city/Apple Valley' (found at least ' ')"
     ]
    }
   ],
   "source": [
    "#  Scrape the content of each link\n",
    "data1=[]\n",
    "for url in final_list:\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    soup = BS(webpage,'html.parser')\n",
    "    allrows=soup.find_all('p', attrs={\"class\":\"card-text font-size-midr line-height-xl\"})\n",
    "    b=[]\n",
    "    for cell in allrows:\n",
    "        b.append(cell.get_text())\n",
    "    data1.append(b)\n",
    "len(data1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "df4=pd.DataFrame(data1)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevate columns\n",
    "df6=df4.drop(columns=[2,3,6,7,17,21,26])\n",
    "df6.iloc[:,15:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df6.rename(columns={0:'MLS', 1:'MLS Source'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df6.rename(columns={4:'Bathrooms',5:'Kitchen',8:'Fireplace',\n",
    "                   9:'Flooring',10:'Laundry', 11:'Cooling',\n",
    "                   12:'Heating',13:'Roof',14:'Foundation',\n",
    "                   15:'Pool',16:'Style',18:'Garage/Parking',19:'Elementary District',\n",
    "                   20:'High School District',22:'Sewer',23:'Water',24:'HOA Fee',25:'Complex Amenities'}, inplace=True)\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all sub dataframes into one dataframe\n",
    "result = pd.concat([df1, df2, df3,df6], axis=1, join='inner')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "writer=pd.ExcelWriter(\"RealEstateData_PleasantonCA_2020_Final.xlsx\")\n",
    "result.to_excel(writer, \"Pleasanton\", index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Pleasanton CA 2020 Data\"\n",
    "df=pd.read_excel(\"RealEstateData_PleasantonCA_2020_Final.xlsx\")\n",
    "\n",
    "# Select Full Address, Bed, Bath, Year Built, Sqft, House Type, Sold Price\n",
    "df1=df.loc[:,[\"Full Address\",\"Sold Price\",\"Bed\",\"Bath\",\"House Type\",\"Year Built\",\"Sqft\"]]\n",
    "print(df1.head())\n",
    "print(df1.shape)\n",
    "\n",
    "# Extract street name, city, and statezip to its desiginated column\n",
    "\n",
    "df1[['Street','City','StateZip']]=df1['Full Address'].str.split(\",\",n=2,expand=True)\n",
    "df1.head()\n",
    "\n",
    "# Extract State and zip code to its designated column\n",
    "df1[[\"NA\",\"State\",\"Zip Code\"]]=df1['StateZip'].str.split(\" \",n=2,expand=True)\n",
    "df1.head()\n",
    "\n",
    "# Select Street, City, Zip Code, Bed, Bath, Year Built, Sqft, House Type, Sold Price\n",
    "df_wang=df1.loc[:,[\"Street\",\"City\",\"Zip Code\",\"Bed\",\"Bath\",\"Year Built\", \"Sqft\",\"House Type\",\"Sold Price\"]]\n",
    "df_wang.head()\n",
    "\n",
    "# change datatype for \"Bed\" to String\n",
    "df_wang['Bed']=df_wang['Bed'].astype(float)\n",
    "df_wang['Bath']=df_wang['Bath'].astype(float)\n",
    "\n",
    "# change datatype for \"Sqft\" to Float\n",
    "# Step1: we need to remove the \",\" among the digit first before convertion\n",
    "df_wang['Sqft'] = df_wang['Sqft'].str.replace(r',', '')\n",
    "df_wang.head()\n",
    "\n",
    "# Step2: change datetype from string to float\n",
    "df_wang['Sqft']=df_wang['Sqft'].astype(float)\n",
    "df_wang.info()\n",
    "\n",
    "# change datatype for \"Sold Price\" to Float\n",
    "# Step1: we need to remove the \",\" among the digit first before convertion\n",
    "df_wang['Sold Price'] = df_wang['Sold Price'].str.replace(r',', '')\n",
    "df_wang.head()\n",
    "\n",
    "# Step2: we need to remove the \"$\" before convertion\n",
    "df_wang['Sold Price'] = df_wang['Sold Price'].str.replace(r'$', '')\n",
    "df_wang.head()\n",
    "\n",
    "df_wang['Sold Price']=df_wang['Sold Price'].astype(float)\n",
    "df_wang.info()\n",
    "\n",
    "df_wang.to_csv(\"PleasantonCA.csv\",index=False)\n",
    "df_p=pd.read_csv(\"PleasantonCA.csv\")\n",
    "df_p.head()\n",
    "\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94588-3762','94588')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94566-7248','94566')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94566-7132','94566')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'CA 94566','94566')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94588-9999','94588')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94566-2237','94566')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94588-4452','94588')\n",
    "df_p['Zip Code']=df_p['Zip Code'].str.replace(r'94566-7433','94566')\n",
    "\n",
    "df_p.to_csv(\"PleasantonCA.csv\",index=False)\n",
    "df_p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Method & Data Cleaning 2 - Santa Clara City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# get the sold price in a single webpage of one house\n",
    "def get_price(soup1):\n",
    "    sold_price=soup1.findAll('h4',attrs={\"class\":\"font-weight-bold h font-size-xl line-height-lg\"})\n",
    "    price=sold_price[0].get_text()\n",
    "    return price\n",
    "def get_addr(soup1):\n",
    "    address=soup1.findAll('h2',attrs={\"class\":\"font-weight-bold h font-size-xl line-height-lg\"})\n",
    "    addr=address[0].get_text()\n",
    "    return addr\n",
    "def get_bed(soup1):\n",
    "    container=soup1.findAll('th',attrs={\"class\":\"text-xs-center\"})\n",
    "    bed=container[0].get_text()\n",
    "    return bed\n",
    "def get_bath(soup1):\n",
    "    ba=soup1.findAll('meta',attrs={\"property\":\"mlsl_fb:baths\"})\n",
    "    if(len(ba)==0):\n",
    "        return 'NA'\n",
    "    bath=ba[0].attrs['content']\n",
    "    return bath\n",
    "def get_year(soup1):\n",
    "    year=soup1.findAll('th',attrs={\"class\":\"text-xs-center pr-0\"})\n",
    "    year_built=year[0].get_text()\n",
    "    return year_built\n",
    "def get_sqft(soup1):\n",
    "    sqft=soup1.findAll('th',attrs={\"class\":\"text-xs-center\"})\n",
    "    sqft=sqft[2].get_text()\n",
    "    return sqft\n",
    "def get_house_info(h_url):\n",
    "    req = Request(h_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage1 = urlopen(req).read()\n",
    "    soup1 = BS(webpage1,'html.parser')\n",
    "    \n",
    "    price = get_price(soup1)\n",
    "    address = get_addr(soup1)\n",
    "    bed = get_bed(soup1)\n",
    "    bath = get_bath(soup1)\n",
    "    year = get_year(soup1)\n",
    "    sqft = get_sqft(soup1)\n",
    "    #dictionary\n",
    "    house_info={}\n",
    "    house_info['price']=price\n",
    "    house_info['bed']=bed\n",
    "    house_info['address']=address\n",
    "    house_info['bath']=bath\n",
    "    house_info['year_built']=year\n",
    "    house_info['sqft']=sqft\n",
    "    return house_info\n",
    "#call this function can get all the house links (about 31 house links)of each page.\n",
    "def get_house_url(sc_url):\n",
    "    santa_clara_url = sc_url\n",
    "    req = Request(santa_clara_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    soup = BS(webpage,'html.parser')\n",
    "  \n",
    "    onepage=soup.findAll('a',attrs={\"class\":\"search-nav-link prerender\"})\n",
    "    pages=[]\n",
    "    for link in onepage:\n",
    "        onehouse=link.attrs['href']\n",
    "        onehouse=\"https://www.mlslistings.com\"+onehouse\n",
    "        if onehouse not in pages:\n",
    "            pages.append(onehouse)\n",
    "    return pages\n",
    "# put all the links of all house (about 124)into a list\n",
    "web=['https://www.mlslistings.com/Search/Result/87ed05c1-6e03-4a5b-a103-d7feeccf54d5/1',\n",
    "     'https://www.mlslistings.com/Search/Result/87ed05c1-6e03-4a5b-a103-d7feeccf54d5/2?view=list',\n",
    "    'https://www.mlslistings.com/Search/Result/87ed05c1-6e03-4a5b-a103-d7feeccf54d5/3?view=list',\n",
    "     'https://www.mlslistings.com/Search/Result/87ed05c1-6e03-4a5b-a103-d7feeccf54d5/4?view=list']\n",
    "\n",
    "weburl=[]\n",
    "\n",
    "for x in web:\n",
    "    weburl.extend(get_house_url(x))\n",
    "len(weburl)\n",
    "\n",
    "#may need several minutes to run\n",
    "data1=[]\n",
    "for x in weburl:\n",
    "    house_info=get_house_info(x)\n",
    "    data1.append(house_info)\n",
    "data1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame(data1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning \n",
    "santa_clara=pd.DataFrame()\n",
    "street = df['address'].map(lambda x : x.split(',')[0])\n",
    "santa_clara['street'] = street\n",
    "santa_clara['city'] = 'Santa Clara'\n",
    "zip_code = df['address'].map(lambda x : x.split(',')[2][3:])\n",
    "zip_code\n",
    "santa_clara['zip_code'] =zip_code\n",
    "\n",
    "santa_clara['bed']=df['bed']\n",
    "\n",
    "new_bath= df['bath'].map(lambda x : x.split()[0])\n",
    "santa_clara['bath'] = new_bath\n",
    "\n",
    "new_price = df['price'].map(lambda x : x.split()[0])\n",
    "new_price\n",
    "santa_clara['sold_price'] = new_price\n",
    "santa_clara['year_built'] = df['year_built']\n",
    "\n",
    "\n",
    "def get_house_type(x):\n",
    "    if \"Family\" in x:\n",
    "        return \"Single Family Residence\"\n",
    "    elif \"Condo\" in x or \"condo\" in x:\n",
    "        return \"condominium\"\n",
    "    elif \"Townhouse\" in x:\n",
    "        return \"Townhouse\"\n",
    "    else :\n",
    "        return \"NA\"\n",
    "santa_clara['house_type'] =df['price'].map(get_house_type)\n",
    "\n",
    "def price_to_int(x):\n",
    "    try:\n",
    "        return int(x.replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "    except:\n",
    "        return \"NA\"\n",
    "santa_clara['sold_price'] = santa_clara['sold_price'].map(price_to_int)\n",
    "\n",
    "def sqft_to_float(x):\n",
    "    try:\n",
    "        return int(x.replace(\",\",\"\"))\n",
    "    except:\n",
    "        return \"NA\"\n",
    "santa_clara['sqft'] = df['sqft'].map(sqft_to_float)\n",
    "\n",
    "santa_clara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara['street']=santa_clara['street'].astype(str)\n",
    "santa_clara['city']=santa_clara['city'].astype(str)\n",
    "santa_clara['zip_code']=santa_clara['zip_code'].astype(str)\n",
    "santa_clara['house_type']=santa_clara['house_type'].astype(str)\n",
    "santa_clara['year_built']=santa_clara['year_built'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara['bath']=santa_clara['bath'].replace({\"\":\"0\",\"2/1\":\"2.5\",\"2/2\":\"2.5\",\"3/1\":\"3.5\",\"3/2\":\"3.5\",\"4/1\":\"4.5\",\"1/1\":\"1.5\"})\n",
    "santa_clara['bath']=santa_clara['bath'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara['bath']=santa_clara['bath'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara['bed']=santa_clara['bed'].replace(\"\",\"0\")\n",
    "santa_clara['bed']=santa_clara['bed'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara['sold_price']=santa_clara['sold_price'].replace(\"NA\",\"0\")\n",
    "santa_clara['sold_price']=santa_clara['sold_price'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara['sqft']=santa_clara['sqft'].astype(float)\n",
    "santa_clara.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "santa_clara.to_csv('santa_clara1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Method & Data Cleaning 3 -  Sunnyvale City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Street</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Bed</th>\n",
       "      <th>Bath</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Sqft</th>\n",
       "      <th>House Type</th>\n",
       "      <th>Sold Price</th>\n",
       "      <th>Sold Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>636 Smoke Tree Way</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94086</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1971</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>1950000.0</td>\n",
       "      <td>2020-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1618 Kennewick Dr</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94087</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1959</td>\n",
       "      <td>2590.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>2650000.0</td>\n",
       "      <td>2020-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1660 Belleville Way</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1985</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>2270000.0</td>\n",
       "      <td>2020-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>214 Twinlake Dr</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94089</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1957</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>1635000.0</td>\n",
       "      <td>2020-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>165 Florence St</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94086</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1935</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>1880000.0</td>\n",
       "      <td>2020-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1303 Selo Dr</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94087</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1949</td>\n",
       "      <td>913.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>580 Ahwanee Ave</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94087</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>1182.0</td>\n",
       "      <td>Manufactured In Park</td>\n",
       "      <td>185000.0</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1351 Wright Ave</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94087</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1953</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>Single Family Residence</td>\n",
       "      <td>1930000.0</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>704 San Conrado Ter</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94085</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1986</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1111 Morse Ave</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>94089</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>Manufactured In Park</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Street       City Zip Code  Bed  Bath Year Built    Sqft  \\\n",
       "1     636 Smoke Tree Way  Sunnyvale    94086  3.0   2.0       1971  1685.0   \n",
       "2      1618 Kennewick Dr  Sunnyvale    94087  5.0   3.0       1959  2590.0   \n",
       "3    1660 Belleville Way  Sunnyvale    94087  4.0   3.0       1985  2800.0   \n",
       "4        214 Twinlake Dr  Sunnyvale    94089  4.0   3.0       1957  1125.0   \n",
       "5        165 Florence St  Sunnyvale    94086  3.0   2.0       1935  1598.0   \n",
       "..                   ...        ...      ...  ...   ...        ...     ...   \n",
       "112         1303 Selo Dr  Sunnyvale    94087  3.0   1.0       1949   913.0   \n",
       "113      580 Ahwanee Ave  Sunnyvale    94087  3.0   2.0       1997  1182.0   \n",
       "114      1351 Wright Ave  Sunnyvale    94087  3.0   2.0       1953  1526.0   \n",
       "115  704 San Conrado Ter  Sunnyvale    94085  2.0   2.0       1986  1078.0   \n",
       "116       1111 Morse Ave  Sunnyvale    94089  3.0   2.0       2010  1680.0   \n",
       "\n",
       "                  House Type  Sold Price  Sold Date  \n",
       "1    Single Family Residence   1950000.0 2020-03-13  \n",
       "2    Single Family Residence   2650000.0 2020-03-13  \n",
       "3    Single Family Residence   2270000.0 2020-03-12  \n",
       "4    Single Family Residence   1635000.0 2020-03-12  \n",
       "5    Single Family Residence   1880000.0 2020-03-11  \n",
       "..                       ...         ...        ...  \n",
       "112  Single Family Residence   1500000.0 2020-01-02  \n",
       "113     Manufactured In Park    185000.0 2020-01-02  \n",
       "114  Single Family Residence   1930000.0 2020-01-02  \n",
       "115                Townhouse    900000.0 2020-01-02  \n",
       "116     Manufactured In Park    245000.0 2020-01-01  \n",
       "\n",
       "[116 rows x 10 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_page(url):\n",
    "  import requests\n",
    "  from bs4 import BeautifulSoup\n",
    "  headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "  r = requests.get(url, headers=headers)\n",
    "  soup = BeautifulSoup(r.text, 'html.parser')\n",
    "  return soup\n",
    "\n",
    "def get_card_info(card):\n",
    "  address = card.select('.listing-address')[0].text\n",
    "  price = card.select('.listing-price')[0].text  \n",
    "  bedrooms = card.select('.listing-beds .info-item-value')[1].text # first one is MLS\n",
    "  bathrooms = card.select('.listing-baths .info-item-value')[0].text\n",
    "  sqft = card.select('.listing-sqft .info-item-value')[0].text\n",
    "  year = card.select('.listing-sqft .info-item-value')[1].text\n",
    "  listing_type = card.select('div.listing-type')[0].text.strip()\n",
    "  sold_date = card.select('.status-closed')[0].text\n",
    "\n",
    "  return {\n",
    "    'address': address,\n",
    "    'price': price,\n",
    "    'bedrooms': bedrooms,\n",
    "    'bathrooms': bathrooms,\n",
    "    'sqft': sqft,\n",
    "    'year_built': year,\n",
    "    'listing_type': listing_type,\n",
    "    'sold_date': sold_date\n",
    "  }\n",
    "\n",
    "def get_cards(page):\n",
    "  return page.select('.card.card-block')\n",
    "\n",
    "# print(get_card_info(card))\n",
    "\n",
    "raw_infos = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "  page = get_page('https://www.mlslistings.com/Search/Result/3e3e773c-54de-40ad-a786-dd6b4a5828cc/' + str(i))\n",
    "   # url may change by the Web\n",
    "  cards = get_cards(page)\n",
    "  for card in cards:\n",
    "    try:\n",
    "      raw_infos.append(get_card_info(card))\n",
    "    except:\n",
    "      continue\n",
    "       # In case there are cards info is inadequate, return continue to ignore this card info. \n",
    "        \n",
    "\n",
    "        # Clean address data use usaddress package\n",
    "def parse_address(full_address):\n",
    "  import usaddress\n",
    "  parsed, tag = usaddress.tag(full_address)\n",
    "  street = '{} {}'.format(parsed['AddressNumber'], parsed['StreetName'])\n",
    "  if 'StreetNamePostType' in parsed:\n",
    "      street += ' ' + parsed['StreetNamePostType']\n",
    "  city = parsed['PlaceName']\n",
    "  state = parsed['StateName']\n",
    "  zipcode = parsed['ZipCode']\n",
    "  return street, city, state, zipcode\n",
    "\n",
    "# Clean each of the attributes into a specific format\n",
    "def process_bath(bathrooms):\n",
    "  split = bathrooms.split('/')\n",
    "  if len(split) == 1:\n",
    "    return float(split[0])\n",
    "  else:\n",
    "    return float(split[0]) + float(split[1]) / 2\n",
    "\n",
    "def process_date(sold_date):\n",
    "    import pandas as pd\n",
    "    sold_date_parsed = sold_date.replace('Sold on ', '')\n",
    "    return pd.to_datetime(sold_date_parsed)\n",
    "\n",
    "def process_bedrooms(bedrooms):\n",
    "  return float(bedrooms)\n",
    "\n",
    "def process_price(price):\n",
    "  return float(price.replace('$', '').replace(',', ''))\n",
    "\n",
    "def process_sqft(sqft):\n",
    "  return float(sqft.replace(',', ''))\n",
    "\n",
    "def process_year(year):\n",
    "  return str(year)\n",
    "\n",
    "def process_infos(raw_infos):\n",
    "  import usaddress\n",
    "  infos = []\n",
    "  for raw in raw_infos:\n",
    "    address = raw['address']\n",
    "    parsed, tag = usaddress.tag(address)\n",
    "    if tag != 'Street Address':\n",
    "        continue\n",
    "    try:\n",
    "        street, city, state, zipcode = parse_address(address)\n",
    "    except:\n",
    "        print('ai ya')\n",
    "        print(address)\n",
    "    info = {}\n",
    "    info['Street'] = street\n",
    "    info['City'] = city\n",
    "    info['Zip Code'] = zipcode\n",
    "    info['Bed'] = process_bedrooms(raw['bedrooms'])\n",
    "    info['Bath'] = process_bath(raw['bathrooms'])\n",
    "    info['Year Built'] = process_year(raw['year_built'])\n",
    "    info['Sqft'] = process_sqft(raw['sqft'])\n",
    "    info['House Type'] = raw['listing_type']\n",
    "    info['Sold Price'] = process_price(raw['price'])\n",
    "    info['Sold Date'] = process_date(raw['sold_date'])\n",
    "    infos.append(info)\n",
    "  return infos\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "infos = process_infos(raw_infos)\n",
    "print(len(infos))\n",
    "df = pd.DataFrame(infos, index = range(1,117))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
